[{"content":" I was feeling particularly nostalgic about my childhood in Paris tonight, and I remembered this speech I wrote when I was 17. It is pretty silly, a bit cringeworthy, but I wanted to discreetly share it here.\n Do you remember your first love? I just turned 6 when I met mine. Don\u0026rsquo;t worry, this speech is not about some sappy romance. I can\u0026rsquo;t tell when, how or why exactly I fell in love with Paris, but by the time I realized, I saw Paris in all my habits, my hobbies, myself.\nI come from a little town in Japan in the middle of nowhere, that nobody has heard of. So moving to Paris, was as you can imagine, quite the culture shock. I still remember the first time I came to Paris. I fell asleep during the flight and when I woke up in the hotel, the first thing I saw was the elegant streets and fancy buildings of Trocadéro outside my window. The first thing I said was, \u0026ldquo;Am I in Cinderella\u0026rsquo;s castle?\u0026rdquo;. For 6-year old me back then, I felt like I fell in a Disney movie.\nI guess moving to Paris is my own Cinderella story, and I am not simply talking about my wardrobe transformation after my mom discovered Les Soldes. France is my Prince Charming who gave me the wealth of art and culture.\n  Les Soldes - bi-annual sale in France\n  The closest place to a museum they had near my Japanese hometown is called the \u0026ldquo;bridge museum\u0026rdquo; which was a museum in Akashi, a small city near my town, about a bridge we had there. Yes, just about a single bridge. Paris not only has over 130 museums, but what I find so wonderful about this city is how much they encourage young people to discover art. Most museums are free for those under 18, and to everyone every first Sunday of the month. My mother took to a museum every month and even though at the time, neither of us had any knowledge about all the paintings of the Virgin Mary at the Louvre that looked identical to us, I really believe this early exposure to art was crucial to the appreciation for art I have today.\n  Akashi Kaikyo Bridge - the second longest suspension bridge in the world!\n  The first summer I went back to Japan after moving to Paris, we visited one of my mom\u0026rsquo;s old friends. There, I noticed a fridge magnet of a sunflower painting she had, and told her that I really like this Van Gogh painting. This simple remark was quite the surprise to both my mom and her friend, it was unexpected that a 6-year old in some remote Japanese town recognizes Van Gogh. I was actually the first to recognize Van Gogh out of any of her visitors. My mom always tells me this story, saying how lucky I am to have grown up in such an environment, and how much Paris could change you in just one year.\n  Sunflowers by Vincent Van Gogh\n  Another thing I admire about France is that everyone has the right to healthcare and education, regardless of background. Paris has many public cultural centers offering classes with very accessible prices, that vary with your family\u0026rsquo;s income. Even for those that are better off, a week of tennis camp only cost 25€ a day, with lunch included! This is unimaginable where I come from, where only wealthy people learn to play tennis. With so many things I could do here, so many I wanted to do, and nothing stopping me, I just couldn\u0026rsquo;t help ending up learning over 7 instruments, trying over 20 sports. Oops.\nOne final way Paris left its impression in me is the voice it gave me. Parisians are known for always complaining. This culture is quite different from how back in Japan, we are taught to never question order and you must conform to society. But here, the protests and strikes that happen so often, that the metros never function, inspired and taught me it is okay to not follow the crowd, you should speak up no matter how small your voice seems. In Paris, I marched for gender equality. I marched for climate change, I marched for LGBT rights, I marched for everything I believe in. I marched for myself.\n  Paris Climate March 2018\n  Looking at all the ways I am me today, and I am so happy to be, Paris is really the one fateful meeting that shaped my identity. Thank you, for showing me art that gave me new pair of eyes to discover the world. Thank you, for teaching me music with which I make so many joyful memories and meet wonderful people. Thank you, for opening all the doors for me and letting me be myself.\nI want end on a quote by Hemingway: \u0026ldquo;If you are lucky enough to have lived in Paris as a young man, then wherever you go for the rest of your life, it stays with you, for Paris is a moveable feast.\u0026rdquo;\n","permalink":"https://kayoyin.github.io/blog/post/paris/","summary":"I was feeling particularly nostalgic about my childhood in Paris tonight, and I remembered this speech I wrote when I was 18. It is a little silly, but I wanted to discreetly share it here.","title":"A Love Letter to Paris"},{"content":"Project and write-up by Kevin Chen, Maya Shen, Kayo Yin and Kenneth Zheng (in alphabetical order). Blog post adapted from the write-up.\n  Introduction In this project, we visualize songs using images that pulsate and move along with the music using Compositional Pattern Producing Networks (CPPN). In addition, the images we use are recognizable or reminiscent of the songs themselves to help viewers feel a connection with the music and gain a deeper appreciation for it.\nThis project was done for the Art and Machine Learning course offered at Carnegie Mellon University in Spring 2022. This blog post accompanies the Colab notebook here which contains the pipeline to generate visualizations using NeuroMV.\nThe above figure illustrates the NeuroMV pipeline. In this blog post, we will walk through each step of the pipeline.\nInstrument Separation First, we can optionally separate the instruments in the input audio, to create visualizations for each separate instrument. To do so, we use a source separation model from Spleeter. After separating, for example, the voice from the accompaniment, we can feed the voice audio file and the accompaniment audio file individually into NeuroMV to obtain two separate visualizations. The different visualizations can be recombined to be played side-by-side with the original audio.\nFeature Extraction After obtaining the desired input audio file, we perform feature extraction to obtain a latent vector representation $z$ of the musical features at each timestep corresponding to a video frame at a rate of 30 fps. To do this, we use a Mel spectrogram with a hop size of 1/30th of a second and a window size of double that (1/15th of a second).\nA spectrogram allows us to get the energy at various frequency bins at each timestep, which is a good representation of how we hear music. Instead of a traditional (linear) spectrogram, we use one that scales the frequency bins to the Mel scale, which better corresponds to human perception. By changing the number of Mel filters, we are also able to effectively control the size of the latent vector $z$.\nCPPN Model To generate each frame of the music visualization, we use Compositional Pattern Producing Networks (CPPN). Essentially, a CPPN models a given image by approximating it as a mapping between pixel positions $(x,y)$ to pixel colors $(r,g,b)$. Once this mapping has been found, the CPPN model can easily scale and stretch around the image by adjusting the input.\nFollowing previous work using CPPNs to generate abstract art, we use a neural network with $5$ hidden layers as our image generation model, using tanh activations for the hidden layers (which produce a painterly style with soft edges in the generated images), and a sigmoid activation in the final layer to clamp the RGB output values between $0$ and $1$.\n  CPPN model architecture\n  In addition to the $(x,y)$ coordinates, we feed a latent vector $z$ encoding music features to the CPPN for each frame, inspired by this blog post. Because the CPPN is a continuous function, if we modify $z$ by only a small amount, the output video frame also varies only slightly, whereas a large modification of $z$ leads to a large change in the output frame. Thus, the variations in music encoded in $z$, both in pitch and amplitude, can be reflected in the output video through variations in the video frames.\nBy simply initializing all layer weights randomly from a standard Gaussian distribution, our CPPN model can already generate very interesting abstract images. For example, below is the first frame from a test visualization we did with a randomly initialized CPPN with the song Spectrum by Zedd (full video here).\n  An example of a frame generated using a randomly initialized CPPN\n  Pre-training with Images To add an element of customizability to our music visualizer, we can pre-train the CPPN model to match a reference image. This can be achieved by simply converting a given image to a training dataset, where the inputs are the $(x,y)$ position of each pixel in the image and the labels are their $(r,g,b)$ pixel values. While the model will not be able to perfectly capture all the small details in a training image, it is able to capture the general shapes and colors present, which can influence the look and feel of the resulting visualization.\nBelow is an example of a visualization frame generated with this method, using the song Liability by Lorde as the music track, and the album cover art of its album Melodrama as the training image. You can see the full visualization for this example here.\n  Left: reference image (album cover of Lorde’s Melodrama), right: image generated by corresponding trained CPPN.\n  We tried using the album cover as a default image for various songs we tested, but also experimented with other methods to retrieve reference images including using ML systems like DALL-E or WOMBO to generate images from lyrics/text descriptions, or simply manually choosing an aesthetic or thematic image.\nGenerating the Video Finally, we use FFmpeg to combine the generated video frames into a single video, and add the accompanying audio to the video. If we generated multiple videos for different instruments, we also use FFmpeg to combine the videos side-by-side.\nConclusion and Future Work Overall, we\u0026rsquo;re very happy with how well CPPN models with Mel spectrograms as a latent representation of music can create visualizations of songs! We are excited to see album covers and other visual representations of our favorite songs move and pulse with the music. By including aspects of songs which aren’t necessarily auditory (e.g. album cover, movie still for a soundtrack) in our visualizations, we hope to bring in novel aspects which may also allow the viewer to have a deeper experience.\nIn the future, it would be interesting if the images themselves can encode other aspects of songs: for example, generating a sequence of images using text-to-image models by feeding in the lyrics, or generate images guided by the audio by using models such as AudioCLIP.\n  Images generated using Dream by WOMBO for “Long Lost” by Lord Huron, “Liability” by Lorde, “Electra Heart” by MARINA, and “Formula” by Labrinth based on song title, artist name, and album cover.\n  Another limitation of our current pipeline is that there is a trade-off between how well the CPPN model reproduces the reference image, and how much variations conditioned on the audio in the video frames we can observe: the longer we train CPPN on the reference image, the more accurate the output image becomes, but the resulting video may become static. Therefore, it would be useful to explore how we can increase the CPPN output accuracy while preserving its sensitivity to the latent music representation.\n","permalink":"https://kayoyin.github.io/blog/post/neuromv/","summary":"In this project, we visualize songs using images that pulsate and move along with the music using Compositional Pattern Producing Networks (CPPN).","title":"NeuroMV: A Neural Music Visualizer"}]